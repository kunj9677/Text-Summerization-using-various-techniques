# Text-Summerization-using-various-techniques


Summarization techniques are used to condense large amounts of text or data into shorter summaries 
while still capturing the most important information. There are different types of summarization 
techniques including extractive and abstractive summarization. 


Extractive summarization involves selecting the most relevant sentences or phrases from the original 
text and presenting them as a summary. This can be done using various techniques such as frequency 
analysis, graph-based algorithms, and machine learning models. 


Abstractive summarization involves generating a summary by understanding the content of the 
original text and using natural language generation techniques to create a new summary. This can be 
done using deep learning models like neural networks and transformer models.



#Techniques used by me:-
1. LexRank: This is an unsupervised graph-based approach to extractive summarization that 
uses the concept of eigenvector centrality to identify important sentences in a text. It creates a 
similarity matrix based on the pairwise cosine similarity between sentences and then applies a 
PageRank-style algorithm to assign importance scores to each sentence.
2. BERT: Bidirectional Encoder Representations from Transformers (BERT) is a pre-trained 
language model that can be fine-tuned for various natural language processing tasks, 
including summarization. It uses a transformer-based neural network architecture to learn 
contextual representations of words and can generate abstractive summaries by predicting the 
most important sentences or phrases in a text.
3. GPT-2: Generative Pre-trained Transformer 2 (GPT-2) is a large-scale transformer-based 
language model trained on a diverse range of internet text. It can be fine-tuned for various 
natural language processing tasks, including summarization, by conditioning the model to 
generate a summary of a given text.
4. Luhn: The Luhn algorithm is an extractive summarization technique that ranks sentences 
based on the frequency of important words or phrases. It works by calculating the frequency 
of each word in a text and then assigning a score to each sentence based on the presence of 
these key terms.
5. LSA: Latent Semantic Analysis (LSA) is a statistical natural language processing technique 
that uses a matrix factorization method to identify relationships between words and concepts 
in a text. It can be used for extractive summarization by selecting the sentences that contain 
the most important latent semantic topics in the original text.
6. LDA: Latent Dirichlet Allocation (LDA) is a topic modeling technique used in natural 
language processing to identify the underlying topics or themes in a large corpus of text. It 
works by assuming that each document in the corpus is a mixture of several topics and that 
each topic is a probability distribution over a set of words. LDA can be used for extractive 
summarization by identifying the most important topics in a text and selecting the sentences 
that are most representative of these topics.
